{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b85e285",
   "metadata": {},
   "source": [
    "## Build a RAG App: Part 2 <img src=\"../../images/db-icon.png\" width=25 />\n",
    "In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.\n",
    "\n",
    "This guide extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\n",
    "Here we focus on adding logic for incorporating historical messages. This involves the management of a chat history.\n",
    "\n",
    "We will cover two approaches:\n",
    "- **Chains** - in which we execute at most one retrieval step;\n",
    "- **Agents** - in which we give an LLM discretion to execute multiple retrieval steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9334a3",
   "metadata": {},
   "source": [
    "### LangSmith\n",
    "\n",
    "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n",
    "\n",
    "After you sign up at the link above, make sure to set environment variables to start logging traces:\n",
    "\n",
    "```LANGCHAIN_TRACING_V2=true```<br>\n",
    "```LANGCHAIN_API_KEY=\"...\"```<br>\n",
    "```LANGSMITH_PROJECT=tutorial```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11411048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# load environmental variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745a457",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11b40a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "llm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a372d5",
   "metadata": {},
   "source": [
    "#### Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5685ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks_langchain import DatabricksEmbeddings\n",
    "\n",
    "embeddings = DatabricksEmbeddings(endpoint=\"databricks-gte-large-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b093a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f0525f",
   "metadata": {},
   "source": [
    "#### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39541aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce263457",
   "metadata": {},
   "source": [
    "### Chains\n",
    "\n",
    "In the Part 1 of the RAG tutorial, we represented the user input, retrieved context, and generated answer as separate keys in the state. Conversational experiences can be naturally represented using a sequence of messages. In addition to messages from the user and assistant, retrieved documents and other artifacts can be incorporated into a message sequence via tool messages. This motivates us to represent the state of our RAG application using a sequence of messages. Specifically, we will have\n",
    "- User input as a HumanMessage;\n",
    "- Vector store query as an AIMessage with tool calls;\n",
    "- Retrieved documents as a ToolMessage;\n",
    "- Final response as a AIMessage.\n",
    "\n",
    "This model for state is so versatile that LangGraph offers a built-in version for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e9e988",
   "metadata": {},
   "source": [
    "Leveraging tool-calling to interact with a retrieval step has another benefit, which is that the query for the retrieval is generated by our model. This is especially important in a conversational setting, where user queries may require contextualization based on the chat history. For instance, consider the following exchange:\n",
    "\n",
    "Human: \"What is Task Decomposition?\"\n",
    "\n",
    "AI: \"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model.\"\n",
    "\n",
    "Human: \"What are common ways of doing it?\"\n",
    "\n",
    "In this scenario, a model could generate a query such as \"common approaches to task decomposition\". Tool-calling facilitates this naturally. As in the query analysis section of the RAG tutorial, this allows a model to rewrite user queries into more effective search queries. It also provides support for direct responses that do not involve a retrieval step (e.g., in response to a generic greeting from the user).\n",
    "\n",
    "Let's turn our retrieval step into a tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8386b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c12081",
   "metadata": {},
   "source": [
    "Our graph will consist of three nodes:\n",
    "- A node that fields the user input, either generating a query for the retriever or responding directly;\n",
    "- A node for the retriever tool that executes the retrieval step;\n",
    "- A node that generates the final response using the retrieved context.\n",
    "\n",
    "We build them below. Note that we leverage another pre-built LangGraph component, ToolNode, that executes the tool and adds the result as a ToolMessage to the state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1b53a",
   "metadata": {},
   "source": [
    "#### Node 1: User Input\n",
    "Generate an AIMessage that may include a tool-call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71cfeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e36d5b7",
   "metadata": {},
   "source": [
    "#### Node 2: Execute Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tools = ToolNode([retrieve])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad62c864",
   "metadata": {},
   "source": [
    "#### Node 3: Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf64a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Only use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Keep the answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2545e517",
   "metadata": {},
   "source": [
    "Finally, we compile our application into a single graph object. In this case, we are just connecting the steps into a sequence. We also allow the first query_or_respond step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "app = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b906a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61417337",
   "metadata": {},
   "source": [
    "Let's test our application.\n",
    "\n",
    "Note that it responds appropriately to messages that do not require an additional retrieval step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608f1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"Hello\"\n",
    "\n",
    "for step in app.stream({\"messages\": [{\"role\": \"user\", \"content\": input_message}]}, stream_mode=\"values\",):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb29ce9",
   "metadata": {},
   "source": [
    "And when executing a search, we can stream the steps to observe the query generation, retrieval, and answer generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"What are common methods for Task Decomposition?\"\n",
    "\n",
    "for step in app.stream({\"messages\": [{\"role\": \"user\", \"content\": input_message}]}, stream_mode=\"values\"):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7bfedc",
   "metadata": {},
   "source": [
    "#### Stateful management of chat history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe889be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"What is Task Decomposition?\"\n",
    "\n",
    "for step in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"Can you look up some common ways of doing it?\"\n",
    "\n",
    "for step in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ceb8ec",
   "metadata": {},
   "source": [
    "Note that the query generated by the model in the second question incorporates the conversational context.\n",
    "\n",
    "The [LangSmith](https://smith.langchain.com/public/28e6179f-fc56-45e1-9028-447d76352c14/r) trace is particularly informative here, as we can see exactly what messages are visible to our chat model at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed4343f",
   "metadata": {},
   "source": [
    "### Agents\n",
    "Agents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above \"chain\", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search.\n",
    "\n",
    "Below we assemble a minimal RAG agent. Using LangGraph's pre-built ReAct agent constructor, we can do this in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf2410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent = create_react_agent(llm, [retrieve], checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c065253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7354f9c7",
   "metadata": {},
   "source": [
    "The key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information.\n",
    "\n",
    "Let's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"def234\"}}\n",
    "\n",
    "input_message = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb02960a",
   "metadata": {},
   "source": [
    "Note that the agent:\n",
    "- Generates a query to search for a standard method for task decomposition;\n",
    "- Receiving the answer, generates a second query to search for common extensions of it;\n",
    "- Having received all necessary context, answers the question.\n",
    "\n",
    "We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
