{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ee5142",
   "metadata": {},
   "source": [
    "## LangChain Ecosystem\n",
    "\n",
    "### üß† LangChain\n",
    "LangChain is a framework for building applications with LLMs (Large Language Models). It helps developers connect LLMs with:\n",
    "\n",
    "- Data sources (like PDFs, databases, APIs),\n",
    "- Memory (for conversation or context retention),\n",
    "- Tools (for retrieval, calculation, web browsing, etc.),\n",
    "- Agents (LLM-powered decision-making logic).\n",
    "\n",
    "Think of it as the backbone for creating LLM-based apps. It abstracts the hard parts of dealing with prompts, chains of reasoning, and tool use.\n",
    "\n",
    "### üîÅ LangGraph\n",
    "LangGraph is a library built on top of LangChain that allows you to define stateful, multi-step, looping workflows for LLMs using a graph-based architecture. Use LangGraph when:\n",
    "\n",
    "- You need agents that can loop, retry, or take complex decision paths.\n",
    "- You want clear control flow (nodes and edges).\n",
    "- You‚Äôre building more sophisticated agents (e.g., multi-agent systems, dynamic task workflows).\n",
    "\n",
    "In simple terms: LangChain handles what happens, and LangGraph helps you define how things flow over time.\n",
    "\n",
    "### üîé LangSmith\n",
    "LangSmith is a developer tool for debugging, evaluating, and monitoring LLM apps. It lets you:\n",
    "\n",
    "- Track every LLM call and intermediate step.\n",
    "- Visualize complex agent behaviors.\n",
    "- Run evaluations for different prompts or models.\n",
    "- Monitor performance and usage in production.\n",
    "\n",
    "LangSmith makes it easier to understand, improve, and trust what your LangChain or LangGraph app is doing.\n",
    "\n",
    "### ü§ù How They Complement Each Other\n",
    "\n",
    "|Tool|Purpose|How it Fits In|\n",
    "|----|-------|--------------|\n",
    "|LangChain|Build with LLMs|Core framework ‚Äì makes working with LLMs easier\n",
    "|LangGraph|Control Flow|Adds flexible, graph-based workflows on top of LangChain\n",
    "|LangSmith|Debug + Evaluate|Monitors and debugs LangChain/LangGraph apps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed02ec5",
   "metadata": {},
   "source": [
    "## Introduction to Langchain\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "LangChain simplifies every stage of the LLM application lifecycle:\n",
    "\n",
    "- **Development:** Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\n",
    "- **Productionization:** Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\n",
    "- **Deployment:** Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb4021",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "The LangChain framework consists of multiple open-source libraries.\n",
    "\n",
    "- ```langchain-core:``` Base abstractions for chat models and other components.\n",
    "- ```Integration packages``` (e.g. langchain-openai, langchain-anthropic, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\n",
    "- ```langchain:``` Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
    "- ```langchain-community:``` Third-party integrations that are community maintained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e552845",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Why LangChain?\n",
    "The goal of langchain the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason. While LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem. This page will talk about the LangChain ecosystem as a whole. Most of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ed6b75",
   "metadata": {},
   "source": [
    "#### Features\n",
    "There are several primary needs that LangChain aims to address:\n",
    "\n",
    "1. **Standardized component interfaces:** The growing number of models and related components for AI applications has resulted in a wide variety of different APIs that developers need to learn and use. This diversity can make it challenging for developers to switch between providers or combine components when building applications. LangChain exposes a standard interface for key components, making it easy to switch between providers.\n",
    "\n",
    "2. **Orchestration:** As applications become more complex, combining multiple components and models, there's a growing need to efficiently connect these elements into control flows that can accomplish diverse tasks. Orchestration is crucial for building such applications.\n",
    "\n",
    "3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them. Furthermore, the pace of development can become rate-limited by the paradox of choice. For example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. Observability and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61154b65",
   "metadata": {},
   "source": [
    "#### Standardized component interfaces\n",
    "LangChain provides common interfaces for components that are central to many AI applications. As an example, all chat models implement the BaseChatModel interface. This provides a standard way to interact with chat models, supporting important but often provider-specific features like tool calling and structured outputs.\n",
    "\n",
    "##### Example: chat models\n",
    "Many model providers support tool calling, a critical feature for many applications (e.g., agents), that allows a developer to request model responses that match a particular schema. The APIs for each provider differ. LangChain's chat model interface provides a common way to bind tools to a model in order to support tool calling:\n",
    "\n",
    "```python\n",
    "# Tool creation\n",
    "tools = [my_tool]\n",
    "# Tool binding\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f411c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Similarly, getting models to produce structured outputs is an extremely common use case. Providers support different approaches for this, including JSON mode or tool calling, with different APIs. LangChain's chat model interface provides a common way to produce structured outputs using the with_structured_output() method:\n",
    "\n",
    "```python\n",
    "# Define schema\n",
    "schema = ...\n",
    "# Bind schema to model\n",
    "model_with_structure = model.with_structured_output(schema)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206dad64",
   "metadata": {},
   "source": [
    "##### Example: retrievers\n",
    "In the context of RAG and LLM application components, LangChain's retriever interface provides a standard way to connect to many different types of data services or databases (e.g., vector stores or databases). The underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the runnable interface, meaning they can be invoked in a common manner.\n",
    "\n",
    "```python\n",
    "documents = my_retriever.invoke(\"What is the meaning of life?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a73a78",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Orchestration (LangGraph)\n",
    "While standardization for individual components is useful, we've increasingly seen that developers want to combine components into more complex applications. This motivates the need for orchestration. There are several common characteristics of LLM applications that this orchestration layer should support:\n",
    "\n",
    "Complex control flow: The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\n",
    "Persistence: The application needs to maintain short-term and / or long-term memory.\n",
    "Human-in-the-loop: The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\n",
    "The recommended way to orchestrate components for complex applications is LangGraph. LangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges. LangGraph comes with built-in support for persistence, human-in-the-loop, memory, and other features. It's particularly well suited for building agents or multi-agent applications. Importantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph without using LangChain components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d9bce",
   "metadata": {},
   "source": [
    "### Observability and Evaluation (LangSmith)\n",
    "The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality tracing and evaluations can help you rapidly answer these types of questions with confidence. LangSmith is our platform that supports observability and evaluation for AI applications. See our conceptual guides on evaluations and tracing for more details."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
