{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Simple LLM Application <img src=\"../../images/db-icon.png\" width=25 />\n",
    "\n",
    "In this quickstart we'll build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - a single LLM call plus some prompting.  \n",
    "\n",
    "This notebook is based on this [tutorial](https://python.langchain.com/docs/tutorials/llm_chain/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangSmith\n",
    "\n",
    "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n",
    "\n",
    "After you sign up at the link above, make sure to set environment variables to start logging traces:\n",
    "\n",
    "```LANGCHAIN_TRACING_V2=true```<br>\n",
    "```LANGCHAIN_API_KEY=\"...\"```<br>\n",
    "```LANGSMITH_PROJECT=tutorial```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# load environmental variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "\n",
    "First up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For this example, we will using an LLM available on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "#     task=\"text-generation\",\n",
    "#     max_new_tokens=512,\n",
    "#     do_sample=False,\n",
    "#     repetition_penalty=1.03\n",
    "# )\n",
    "\n",
    "# model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "model = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the ```.invoke()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Provide a translation of the following from English into French.\"),\n",
    "    HumanMessage(content=\"Hello, how are you?\")\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "\n",
    "Notice that the response from the model is an AIMessage. This contains a string response along with other metadata about the response. Oftentimes we may just want to work with the string response. We can parse out just this response by using a simple output parser.\n",
    "\n",
    "One way to use the parser is to call it separately. For example, we could save the result of the language model call and then pass it to the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "We are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\n",
    "\n",
    "Prompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\n",
    "\n",
    "Let's create a prompt template here. It will take in two user variables:\n",
    "\n",
    "- language: The language to translate text into\n",
    "- text: The text to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following text into {language}.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template), \n",
    "        (\"user\", \"{text}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = prompt_template.invoke({\"language\": \"French\", \"text\": \"Hello, how are you?\"})\n",
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"French\", \"text\": \"Hello, how are you?\"})\n",
    "result = model.invoke(prompt)\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Components\n",
    "\n",
    "We can now combine this with the model and the output parser from above using the pipe operator ```|```. This is a simple example of using LangChain Expression Language (LCEL) to chain together LangChain modules. There are several benefits to this approach, including optimized streaming and tracing support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"language\": \"French\", \"text\": \"Hello, how are you?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the LangSmith trace, we can see all three components show up in the LangSmith trace.\n",
    "\n",
    "[LangSmith](https://smith.langchain.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving with LangServe\n",
    "\n",
    "While the first part of this guide was intended to be run in a Jupyter Notebook or script, we can use the same approach and use it to build an application. \n",
    "\n",
    "LangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we'll show how you can deploy your app with LangServe.  To launch the app, run the following command in the terminal:\n",
    "\n",
    "```python \n",
    "python serve_simple_llm.py\n",
    "```\n",
    "\n",
    "Now let's set up a client for programmatically interacting with our service. We can easily do this with the langserve.RemoteRunnable. Using this, we can interact with the served chain as if it were running client-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "try:\n",
    "    remote_chain = RemoteRunnable(\"http://localhost:8000/chain/\")\n",
    "    result = remote_chain.invoke({\"language\": \"french\", \"text\": \"hello, how are you?\"})\n",
    "    print(result)\n",
    "except:\n",
    "    print(\"App not running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial you've learned how to create your first simple LLM application. You've learned how to work with language models, how to parse their outputs, how to create a prompt template, chaining them with LCEL, how to get great observability into chains you create with LangSmith, and how to deploy them with LangServe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
