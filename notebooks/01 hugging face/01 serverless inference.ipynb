{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Model Inference <img src=\"../../images/huggingface.png\" width=30 />\n",
    "\n",
    "Inference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive, running on a dedicated server is an option. The huggingface_hub library provides an easy way to call a service that runs inference for hosted models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serverless Inference API\n",
    "\n",
    "Explore the most popular models for text, image, speech, and more — all with a simple API request. Build, test, and experiment without dedicated infrastructure or setup.  The Hugging Face Inference API exposes models that have large community interest and are in active use.  Active models are based on recent likes, downloads, and usage and therefore deployed models can be swapped without prior notice. The Hugging Face stack aims to keep all the latest popular models warm and ready to use.\n",
    "\n",
    "**- Warm models:** models ready to be used.\n",
    "\n",
    "**- Cold models:** models that are not loaded but can be used.\n",
    "\n",
    "**- Frozen models:** models that currently can’t be run with the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Requests library\n",
    "\n",
    "Using the Requests library for Hugging Face serverless inference involves interacting with Hugging Face's Inference API. \n",
    "\n",
    "**1. API Token:** Obtain an API key from Hugging Face by creating an account and generating a token from your settings.\n",
    "\n",
    "**2. Endpoint:** Identify the endpoint URL for the specific model you wish to use. \n",
    "\n",
    "**3. HTTP Request:** Use the Python requests library to send an HTTP POST request to the model endpoint, including:\n",
    "- The input data in JSON format.\n",
    "- An Authorization header containing the API token.\n",
    "\n",
    "**4. Response Handling:** Parse the JSON response returned by the API, which contains the model's prediction or output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. API Token - load API token environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# 2. Endpoint - define the model endpoint \n",
    "MODEL_ENDPOINT = \"https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "# 3. HTTP Request - define the input data and associated headers and make the request\n",
    "input = {\"inputs\": \"Today is a great day\"}\n",
    "headers = {\"Authorization\": f\"Bearer {os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")}\"}\n",
    "\n",
    "response = requests.post(MODEL_ENDPOINT, headers=headers, json=input)\n",
    "\n",
    "# 4. Response Handling - convert the response and display the results:\n",
    "output = response.json()\n",
    "\n",
    "print(f\"Sentiment of text: '{input[\"inputs\"]}'\")\n",
    "for result in output[0]:\n",
    "    print(f\"{result[\"label\"]}: {round(result['score'] * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Inference Client\n",
    "\n",
    "Using the InferenceClient method for Hugging Face serverless inference provides a more streamlined and Pythonic way to interact with the Hugging Face Inference API. This method comes from the huggingface_hub library, simplifying the process of sending inputs and receiving predictions.\n",
    "\n",
    "**1. Initialise Client:** Use the InferenceClient class from huggingface_hub.\n",
    "\n",
    "**2. API Token:** Authenticate with your Hugging Face API token. The client manages this for requests automatically.\n",
    "\n",
    "**3. Call Model:** Use the .post() or task-specific methods like .text_generation() to send input data to a specified model endpoint.\n",
    "\n",
    "**4. Get Results:** The client processes the response and returns predictions in a usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# 1. Initialise Client\n",
    "client = InferenceClient()\n",
    "\n",
    "# 2. API Token (previously loaded from environmental variables)\n",
    "\n",
    "# 3. Call Model\n",
    "input = \"Today is a great day\"\n",
    "response = client.text_classification(model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\", text=input)\n",
    "\n",
    "#4. Get Results\n",
    "print(f\"Sentiment of text: '{input}'\")\n",
    "for result in response:\n",
    "    print(f\"{result[\"label\"]}: {round(result['score'] * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Async Inference Client\n",
    "\n",
    "The AsyncInferenceClient is an asynchronous version of the InferenceClient provided by the huggingface_hub library. It allows you to make non-blocking requests to Hugging Face's Inference API, which is especially useful in scenarios where you need to handle multiple inference tasks concurrently or optimize for responsiveness in your application.\n",
    "\n",
    "The AsyncInferenceClient leverages Python's asyncio framework to support asynchronous programming. You define async functions and use the await keyword to call the client’s methods.\n",
    "\n",
    "**Concurrency:**\n",
    "\n",
    "Traditional synchronous methods block execution while waiting for the server's response.\n",
    "With AsyncInferenceClient, multiple requests can run simultaneously without waiting for one to complete, making it ideal for handling multiple inputs in parallel.\n",
    "\n",
    "**Performance:**\n",
    "\n",
    "Asynchronous execution helps in reducing the total runtime for applications that need to process a large batch of requests or interact with multiple APIs.\n",
    "\n",
    "**Non-blocking Execution:**\n",
    "\n",
    "In web servers or interactive applications, asynchronous operations prevent blocking the main thread, ensuring smooth user experiences and efficient resource use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import AsyncInferenceClient\n",
    "\n",
    "# 1. Initialise Client\n",
    "client = AsyncInferenceClient()\n",
    "\n",
    "# 2. API Token (previously loaded from environmental variables)\n",
    "\n",
    "# 3. Call Model\n",
    "input = \"Today is a great day\"\n",
    "response = await client.text_classification(\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\", \n",
    "    text=input\n",
    ")\n",
    "\n",
    "#4. Get Results\n",
    "print(f\"Sentiment of text: '{input}'\")\n",
    "for result in response:\n",
    "    print(f\"{result[\"label\"]}: {round(result['score'] * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient()\n",
    "client_async = AsyncInferenceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Audio Classification\n",
    "\n",
    "Audio classification is the task of assigning a label or class to a given audio.\n",
    "\n",
    "Example applications:\n",
    "- Recognizing which command a user is giving\n",
    "- Identifying a speaker\n",
    "- Detecting the genre of a song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import pprint\n",
    "from IPython.display import Audio\n",
    "\n",
    "file_path = \"../../data/audio-classify.wav\"\n",
    "\n",
    "response = client.audio_classification(\n",
    "    file_path,\n",
    "    model=\"speechbrain/google_speech_command_xvector\"\n",
    "    )\n",
    "\n",
    "for data in response:\n",
    "    print(f\"Label: {data[\"label\"]}, Score: {round(data[\"score\"], 2)}\")\n",
    "\n",
    "data, samplerate = sf.read(file_path)\n",
    "Audio(data, rate=samplerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Automatic Speech Recognition\n",
    "\n",
    "Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text.\n",
    "\n",
    "Example applications:\n",
    "- Transcribing a podcast\n",
    "- Building a voice assistant\n",
    "- Generating subtitles for a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../data/audio-asr.flac\"\n",
    "\n",
    "response = client.automatic_speech_recognition(file_path)\n",
    "pprint.pprint(response.text)\n",
    "\n",
    "data, samplerate = sf.read(file_path)\n",
    "Audio(data, rate=samplerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text to Image\n",
    "\n",
    "Generate an image based on a given text prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = client.text_to_image(\n",
    "    image_text,\n",
    "    negative_prompt=\"low resolution, blurry\",\n",
    "    model=\"stabilityai/stable-diffusion-3.5-large\"\n",
    ") \n",
    "\n",
    "display(image.resize((300, 300)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_path = \"../../data/teddy.jpg\"\n",
    " \n",
    "image = Image.open(image_path)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image Segmentation\n",
    "\n",
    "Image Segmentation divides an image into segments where each pixel in the image is mapped to an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEED A GOOD EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image Classification\n",
    "\n",
    "Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.image_classification(\n",
    "    \"../../data/teddy.jpg\",\n",
    "    top_k=3,\n",
    "    model=\"google/vit-base-patch16-224\",\n",
    ")\n",
    "\n",
    "for label in response:\n",
    "    print(f\"Class Label: {label[\"label\"]}, Score: {round(label[\"score\"], 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image-Text to Text\n",
    "\n",
    "Image-text-to-text models take in an image and text prompt and output text. These models are also called vision-language models, or VLMs. The difference from image-to-text models is that these models take an additional text input, not restricting the model to certain use cases like image captioning, and may also be trained to accept a conversation as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.image_to_text(\n",
    "    \"../../data/teddy.jpg\",\n",
    "    model=\"Salesforce/blip-image-captioning-large\"\n",
    ")\n",
    "\n",
    "image_text = response.generated_text\n",
    "print(image_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image to Image\n",
    "\n",
    "Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain.\n",
    "\n",
    "Example applications:\n",
    "- Transferring the style of an image to another image\n",
    "- Colorizing a black and white image\n",
    "- Increasing the resolution of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transformed = await client_async.image_to_image(\n",
    "    \"../../data/teddy.jpg\", \n",
    "    prompt=\"change the cat to a tiger\",\n",
    "    model=\"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
    ")\n",
    "\n",
    "display(image_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Object Detection\n",
    "\n",
    "Object Detection models allow users to identify objects of certain defined classes. These models receive an image as input and output the images with bounding boxes and labels on detected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.object_detection(\n",
    "    \"../../data/teddy.jpg\",\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw, ImageFont\n",
    "import random\n",
    "\n",
    "def display_object_detection(image, objects):\n",
    "    draw_image = image.copy()\n",
    "    draw = ImageDraw.Draw(draw_image)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 12)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "    for object in objects:\n",
    "        label = object.label  \n",
    "        score = object.score\n",
    "        box = object.box\n",
    "\n",
    "        random_color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "        xmin, ymin, xmax, ymax = box['xmin'], box['ymin'], box['xmax'], box['ymax']\n",
    "\n",
    "        draw.rectangle(((xmin, ymin), (xmax, ymax)), outline=random_color, width=3)\n",
    "        text_bbox = draw.textbbox((0, 0), f\"{label} ({score:.2f})\", font=font)\n",
    "        text_width = text_bbox[2] - text_bbox[0]\n",
    "        text_height = text_bbox[3] - text_bbox[1]\n",
    "    \n",
    "        text_x = xmax - text_width - 5  \n",
    "        text_y = ymax - text_height - 5  \n",
    "\n",
    "        text_background = [(text_x, text_y), (text_x + text_width, text_y + text_height)]\n",
    "        draw.rectangle(text_background, fill=random_color)\n",
    "        draw.text((text_x, text_y), f\"{label} ({score:.2f})\", fill=\"white\", font=font)\n",
    "\n",
    "    display(draw_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_object_detection(Image.open(\"../../data/teddy.jpg\"), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question Answering\n",
    "\n",
    "Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"../../data/invoice.png\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.document_question_answering(\n",
    "    image=\"../../data/invoice.png\", \n",
    "    question=\"what is the invoice number?\"\n",
    ")\n",
    "\n",
    "print(f\"Answer: {response[0][\"answer\"]}, Score: {round(response[0][\"score\"], 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Question Answering\n",
    "\n",
    "Table Question Answering (Table QA) is the answering a question about an information on a given table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {\"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"], \"Stars\": [\"36542\", \"4512\", \"3934\"]}\n",
    "\n",
    "client.table_question_answering(\n",
    "    table=table,\n",
    "    query=\"What is the average stars rating?\",\n",
    "    model=\"google/tapas-base-finetuned-wtq\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Generation\n",
    "\n",
    "Generate text based on a prompt.  For a Chat Completion task, which generates a response based on a list of messages, check out the chat-completion task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"The huggingface_hub library is\"\n",
    "response = await client_async.text_generation(\n",
    "    input,\n",
    "    max_new_tokens=15,\n",
    ")\n",
    "\n",
    "print(input + response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Classification\n",
    "\n",
    "Text Classification is the task of assigning a label or class to a given text. Some use cases are sentiment analysis, natural language inference, and assessing grammatical correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.text_classification(\n",
    "    \"I like you.\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "for message in response:\n",
    "    print(f\"Label: {message[\"label\"]}, Score: {round(message[\"score\"], 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Summarization\n",
    "\n",
    "Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. \n",
    "Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington \n",
    "Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in \n",
    "New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a \n",
    "broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). \n",
    "Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\n",
    "\"\"\"\n",
    "\n",
    "result = client.summarization(text)\n",
    "\n",
    "result.summary_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Translation\n",
    "\n",
    "Translation is the task of converting text from one language to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.translation(\n",
    "    \"My name is Wolfgang and I live in Berlin\",\n",
    "    model=\"google-t5/t5-base\"\n",
    ")\n",
    "\n",
    "result.translation_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chat Completion\n",
    "\n",
    "Generate a response given a list of messages in a conversational context, supporting both conversational Language Models (LLMs) and conversational Vision-Language Models (VLMs). This is a subtask of text-generation and image-text-to-text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat_completion(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "    max_tokens=500,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for message in response:\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Token Classification\n",
    "\n",
    "Token classification is a task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.token_classification(\n",
    "    \"My name is Scott and I was born in Toronto, Canada\",\n",
    "    model=\"dslim/bert-base-NER\"\n",
    ") \n",
    "\n",
    "for message in response:\n",
    "    print(f\"Word: {message[\"word\"]}, Entity Group: {message[\"entity_group\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Extraction\n",
    "\n",
    "Feature extraction is the task of converting a text into a vector (often called “embedding”). \n",
    "\n",
    "Example applications:\n",
    "- Retrieving the most relevant documents for a query (for RAG applications).\n",
    "- Reranking a list of documents based on their similarity to a query.\n",
    "- Calculating the similarity between two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.feature_extraction(\n",
    "    \"Today is sunny\",\n",
    "    model=\"thenlper/gte-large\"\n",
    ")\n",
    "\n",
    "print(response[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill Mask\n",
    "\n",
    "Mask filling is the task of predicting the right word (token to be precise) in the middle of a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.fill_mask(\n",
    "    \"The capital of France is [MASK].\", \n",
    "    model=\"bert-base-uncased\",\n",
    "    top_k=2\n",
    ")\n",
    "\n",
    "for result in response:\n",
    "    print(f\"Sequence: {result[\"sequence\"]} Score: {round(result[\"score\"]*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zero Shot Classification\n",
    "\n",
    "Zero-shot text classification is super useful to try out classification with zero code, you simply pass a sentence/paragraph and the possible labels for that sentence, and you get a result. The model has not been necessarily trained on the labels you provide, but it can still predict the correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.zero_shot_classification(\n",
    "    text=\"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!\",\n",
    "    labels=[\"refund\", \"legal\", \"faq\"],\n",
    "    model=\"facebook/bart-large-mnli\"\n",
    ")\n",
    "\n",
    "for message in response:\n",
    "    print(f\"Label: {message[\"label\"]}, Score: {round(message[\"score\"], 2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
