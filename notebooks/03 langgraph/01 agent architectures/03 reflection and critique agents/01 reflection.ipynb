{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f8eb04",
   "metadata": {},
   "source": [
    "## Reflection <img src=\"../../../../images/db-icon.png\" width=25 />\n",
    "In the context of LLM agent building, reflection refers to the process of prompting an LLM to observe its past steps (along with potential observations from tools/the environment) to assess the quality of the chosen actions. This is then used downstream for things like re-planning, search, or evaluation.\n",
    "\n",
    "<img src=\"../../../../images/reflection.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4d7c6",
   "metadata": {},
   "source": [
    "### Generate\n",
    "For our example, we will create a \"5 paragraph essay\" generator. First, create the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520985c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an essay assistant tasked with writing excellent 5-paragraph essays.\"\n",
    "            \" Generate the best essay possible for the user's request.\"\n",
    "            \" If the user provides critique, respond with a revised version of your previous attempts.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "from databricks_langchain import ChatDatabricks\n",
    "llm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\", temperature=0)\n",
    "\n",
    "generate = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f90c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay = \"\"\n",
    "request = HumanMessage(content=\"Write an essay on why the little prince is relevant in modern childhood\"\n",
    "                       )\n",
    "for chunk in generate.stream({\"messages\": [request]}):\n",
    "    print(chunk.content, end=\"\")\n",
    "    essay += chunk.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc3d77b",
   "metadata": {},
   "source": [
    "### Reflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b99f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a teacher grading an essay submission. Generate critique and recommendations for the user's submission.\"\n",
    "            \" Provide detailed recommendations, including requests for length, depth, style, etc.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reflect = reflection_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb9c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection = \"\"\n",
    "for chunk in reflect.stream({\"messages\": [request, HumanMessage(content=essay)]}):\n",
    "    print(chunk.content, end=\"\")\n",
    "    reflection += chunk.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd6136",
   "metadata": {},
   "source": [
    "### Repeat\n",
    "And... that's all there is too it! You can repeat in a loop for a fixed number of steps, or use an LLM (or other check) to decide when the finished product is good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in generate.stream({\"messages\": [request, AIMessage(content=essay), HumanMessage(content=reflection)]}):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa48a88",
   "metadata": {},
   "source": [
    "### Define Graph\n",
    "Now that we've shown each step in isolation, we can wire it up in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ca662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Sequence\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "async def generation_node(state: State) -> State:\n",
    "    return {\"messages\": [await generate.ainvoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "async def reflection_node(state: State) -> State:\n",
    "    # Other messages we need to adjust\n",
    "    cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "    # First message is the original user request. We hold it the same for all nodes\n",
    "    translated = [state[\"messages\"][0]] + [\n",
    "        cls_map[msg.type](content=msg.content) for msg in state[\"messages\"][1:]\n",
    "    ]\n",
    "    res = await reflect.ainvoke(translated)\n",
    "    # We treat the output of this as human feedback for the generator\n",
    "    return {\"messages\": [HumanMessage(content=res.content)]}\n",
    "\n",
    "def should_continue(state: State):\n",
    "    if len(state[\"messages\"]) > 4:\n",
    "        # End after 2 iterations\n",
    "        return END\n",
    "    return \"reflect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f5252",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_edge(START, \"generate\")\n",
    "builder.add_conditional_edges(\"generate\", should_continue)\n",
    "builder.add_edge(\"reflect\", \"generate\")\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd6ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "async for event in graph.astream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Generate an essay on the topicality of The Little Prince and its message in modern life\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    config,\n",
    "):\n",
    "    print(event)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2557268",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatPromptTemplate.from_messages(state.values[\"messages\"]).pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
