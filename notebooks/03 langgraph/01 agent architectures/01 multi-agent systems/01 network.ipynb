{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c10d4c",
   "metadata": {},
   "source": [
    "## Multi-Agent Network <img src=\"../../../../images/openai-icon.png\" width=30 />\n",
    "\n",
    "A single agent can usually operate effectively using a handful of tools within a single domain, but even using powerful models like gpt-4, it can be less effective at using many tools.\n",
    "\n",
    "One way to approach complicated tasks is through a \"divide-and-conquer\" approach: create a specialized agent for each task or domain and route tasks to the correct \"expert\". This is an example of a multi-agent network architecture.\n",
    "\n",
    "This notebook (inspired by the paper [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation, by Wu, et. al.](https://arxiv.org/abs/2308.08155)) shows one way to do this using LangGraph.\n",
    "\n",
    "The resulting graph will look something like the following diagram:\n",
    "\n",
    "<img src=\"../../../../images/multi-agent-network.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47857237",
   "metadata": {},
   "source": [
    "### Define Tools\n",
    "We will define some tools that our agents will use in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd886b4",
   "metadata": {},
   "source": [
    "#### Tavily Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f89cf",
   "metadata": {},
   "source": [
    "#### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71876cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", SyntaxWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8476372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "# Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    " \n",
    "repl = PythonREPL()\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(code: Annotated[str, \"The python code to execute to generate your chart.\"]):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a56cdda",
   "metadata": {},
   "source": [
    "### Create Graph\n",
    "Now that we've defined our tools and made some helper functions, will create the individual agents below and tell them how to talk to each other using LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bbc78c",
   "metadata": {},
   "source": [
    "#### Define Agent Nodes\n",
    "We now need to define the nodes.\n",
    "\n",
    "First, we'll create a utility to create a system prompt for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693127ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_system_prompt(suffix: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{suffix}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e18531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.graph import MessagesState, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano\")\n",
    "\n",
    "def get_next_node(last_message: BaseMessage, goto: str):\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return END\n",
    "    return goto\n",
    "\n",
    "# Research agent and node\n",
    "research_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[tavily_tool],\n",
    "    prompt=make_system_prompt(\"You can only do research. You are working with a chart generator colleague.\"),\n",
    ")\n",
    "\n",
    "\n",
    "def research_node(state: MessagesState) -> Command[Literal[\"chart_generator\", END]]:\n",
    "    result = research_agent.invoke(state)\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"chart_generator\")\n",
    "    # wrap in a human message, as not all providers allow\n",
    "    # AI message at the last position of the input messages list\n",
    "    result[\"messages\"][-1] = HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")\n",
    "    return Command(\n",
    "        update={\n",
    "            # share internal message history of research agent with other agents\n",
    "            \"messages\": result[\"messages\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )\n",
    "\n",
    "\n",
    "# Chart generator agent and node\n",
    "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "chart_agent = create_react_agent(\n",
    "    llm,\n",
    "    [python_repl_tool],\n",
    "    prompt=make_system_prompt(\"You can only generate charts. You are working with a researcher colleague.\"),\n",
    ")\n",
    "\n",
    "\n",
    "def chart_node(state: MessagesState) -> Command[Literal[\"researcher\", END]]:\n",
    "    result = chart_agent.invoke(state)\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"researcher\")\n",
    "    # wrap in a human message, as not all providers allow\n",
    "    # AI message at the last position of the input messages list\n",
    "    result[\"messages\"][-1] = HumanMessage(content=result[\"messages\"][-1].content, name=\"chart_generator\")\n",
    "    return Command(\n",
    "        update={\n",
    "            # share internal message history of chart agent with other agents\n",
    "            \"messages\": result[\"messages\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f64f31f",
   "metadata": {},
   "source": [
    "#### Define the Graph\n",
    "We can now put it all together and define the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"researcher\", research_node)\n",
    "workflow.add_node(\"chart_generator\", chart_node)\n",
    "\n",
    "workflow.add_edge(START, \"researcher\")\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e3eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d58c9f4",
   "metadata": {},
   "source": [
    "#### Invoke the Network\n",
    "With the graph created, you can invoke it! Let's have it chart some stats for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c7264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"First, get the UK's GDP over the past 5 years, then make a line chart of it. \"\n",
    "                \"Once you make the chart, finish.\",\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    # Maximum number of steps to take in the graph\n",
    "    {\"recursion_limit\": 150},\n",
    ")\n",
    "for s in events:\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
