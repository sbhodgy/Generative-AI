{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569872b2",
   "metadata": {},
   "source": [
    "## Multi-agent Supervisor <img src=\"../../../../images/openai-icon.png\" width=30 />\n",
    "The previous example routed messages automatically based on the output of the initial researcher agent.\n",
    "\n",
    "We can also choose to use an LLM to orchestrate the different agents.\n",
    "\n",
    "Below, we will create an agent group, with an agent supervisor to help delegate tasks.\n",
    "\n",
    "<img src=\"../../../../images/multi-agent-supervisor.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068d5cdd",
   "metadata": {},
   "source": [
    "### Create Tools\n",
    "For this example, you will make an agent to do web research with a search engine, and one agent to create plots. Define the tools they'll use below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70e176",
   "metadata": {},
   "source": [
    "#### Tavily Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14762e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4be6c7",
   "metadata": {},
   "source": [
    "#### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb374a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", SyntaxWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef3a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "# Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    " \n",
    "repl = PythonREPL()\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(code: Annotated[str, \"The python code to execute to generate your chart.\"]):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b415b8b",
   "metadata": {},
   "source": [
    "### Create Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00959a37",
   "metadata": {},
   "source": [
    "#### Define Agent Supervisor\n",
    "It will use LLM with structured output to choose the next worker node OR finish processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86379b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import MessagesState, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "members = [\"researcher\", \"coder\"]\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = members + [\"FINISH\"]\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    f\" following workers: {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "\n",
    "\n",
    "class Router(TypedDict):\n",
    "    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "\n",
    "    next: Literal[*options]\n",
    "\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\")\n",
    "# llm = init_chat_model(\"gpt-4o-mini-2024-07-18\", model_provider=\"openai\")\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    next: str\n",
    "\n",
    "\n",
    "def supervisor_node(state: State) -> Command[Literal[*members, \"__end__\"]]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ] + state[\"messages\"]\n",
    "    response = llm.with_structured_output(Router).invoke(messages)\n",
    "    goto = response[\"next\"]\n",
    "    if goto == \"FINISH\":\n",
    "        goto = END\n",
    "\n",
    "    return Command(goto=goto, update={\"next\": goto})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c592b33",
   "metadata": {},
   "source": [
    "#### Define Agent Nodes\n",
    "We now need to define the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47cefae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    llm, tools=[tavily_tool], prompt=\"You are a researcher. DO NOT do any math.\"\n",
    ")\n",
    "\n",
    "\n",
    "def research_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = research_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "\n",
    "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "code_agent = create_react_agent(llm, tools=[python_repl_tool])\n",
    "\n",
    "\n",
    "def code_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = code_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"coder\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e496d",
   "metadata": {},
   "source": [
    "#### Define the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a978b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_node(\"researcher\", research_node)\n",
    "builder.add_node(\"coder\", code_node)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ad22e",
   "metadata": {},
   "source": [
    "### Invoke the Team\n",
    "With the graph created, we can now invoke it and see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfde0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream({\"messages\": [(\"user\", \"What's the square root of 42?\")]}, subgraphs=True):\n",
    "    print(s[1])\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac19190",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Find the latest GDP of New York and California, then calculate the average\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    subgraphs=True,\n",
    "):\n",
    "    print(s[1])\n",
    "    print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
