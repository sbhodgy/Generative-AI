{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab08730",
   "metadata": {},
   "source": [
    "## Reasoning without Observation <img src=\"../../../../images/db-icon.png\" width=25 />\n",
    "In [ReWOO, Xu, et. al](https://arxiv.org/abs/2305.18323), propose an agent that combines a multi-step planner and variable substitution for effective tool use. It was designed to improve on the ReACT-style agent architecture in the following ways:\n",
    "\n",
    "1. Reduce token consumption and execution time by generating the full chain of tools used in a single pass. (ReACT-style agent architecture requires many LLM calls with redundant prefixes (since the system prompt and previous steps are provided to the LLM for each reasoning step)\n",
    "2. Simplify the fine-tuning process. Since the planning data doesn't depend on the outputs of the tool, models can be fine-tuned without actually invoking the tools (in theory).\n",
    "\n",
    "The following diagram outlines ReWOO's overall computation graph:\n",
    "\n",
    "<img src=\"../../../../images/rewoo.png\" width=500 />\n",
    "\n",
    "ReWOO is made of 3 modules:\n",
    "\n",
    "1. ðŸ§ Planner: Generate the plan in the following format:\n",
    "\n",
    "    Plan: [reasoning]<br>\n",
    "    #E1 = Tool[argument for tool]<br>\n",
    "    Plan: [reasoning]<br>\n",
    "    #E2 = Tool[argument for tool with #E1 variable substitution]\n",
    "    ...\n",
    "\n",
    "2. Worker: executes the tool with the provided arguments.\n",
    "3. ðŸ§ Solver: generates the answer for the initial task based on the tool observations.\n",
    "\n",
    "The modules with a ðŸ§  emoji depend on an LLM call. Notice that we avoid redundant calls to the planner LLM by using variable substitution.\n",
    "\n",
    "In this example, each module is represented by a LangGraph node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60f439",
   "metadata": {},
   "source": [
    "### Define Graph State\n",
    "In LangGraph, every node updates a shared graph state. The state is the input to any node whenever it is invoked.\n",
    "\n",
    "Below, we will define a state dict to contain the task, plan, steps, and other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e67bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class ReWOO(TypedDict):\n",
    "    task: str\n",
    "    plan_string: str\n",
    "    steps: List\n",
    "    results: dict\n",
    "    result: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1c222d",
   "metadata": {},
   "source": [
    "### Planner\n",
    "The planner prompts an LLM to generate a plan in the form of a task list. The arguments to each task are strings that may contain special variables (#E{0-9}+) that are used for variable substitution from other task results.\n",
    "\n",
    "Our example agent will have two tools: \n",
    "1. Google - a search engine (in this case Tavily) \n",
    "2. LLM - an LLM call to reason about previous outputs\n",
    "\n",
    "The LLM tool receives less of the prompt context and so can be more token-efficient than the ReACT paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b60e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "model = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae71f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea275a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "For the following task, make plans that can solve the problem step by step. For each plan, indicate \\\n",
    "which external tool together with tool input to retrieve evidence. You can store the evidence into a \\\n",
    "variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...)\n",
    "\n",
    "Tools can be one of the following:\n",
    "(1) Google[input]: Worker that searches results from Google. Useful when you need to find short\n",
    "and succinct answers about a specific topic. The input should be a search query.\n",
    "(2) LLM[input]: A pretrained LLM like yourself. Useful when you need to act with general\n",
    "world knowledge and common sense. Prioritize it when you are confident in solving the problem\n",
    "yourself. Input can be any instruction.\n",
    "\n",
    "For example,\n",
    "Task: Thomas, Toby, and Rebecca worked a total of 157 hours in one week. Thomas worked x\n",
    "hours. Toby worked 10 hours less than twice what Thomas worked, and Rebecca worked 8 hours\n",
    "less than Toby. How many hours did Rebecca work?\n",
    "Plan: Given Thomas worked x hours, translate the problem into algebraic expressions and solve\n",
    "with Wolfram Alpha. #E1 = WolframAlpha[Solve x + (2x - 10) + ((2x - 10) - 8) = 157]\n",
    "Plan: Find out the number of hours Thomas worked. #E2 = LLM[What is x, given #E1]\n",
    "Plan: Calculate the number of hours Rebecca worked. #E3 = Calculator[(2 * #E2 - 10) - 8]\n",
    "\n",
    "Begin! \n",
    "Describe your plans with rich details. Each Plan should be followed by only one #E.\n",
    "\n",
    "Task: {task}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc50c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Who has scored the most goals for the Toronto Maple Leafs and where were they born?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f14e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke(prompt.format(task=task))\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2791cce",
   "metadata": {},
   "source": [
    "### Planner Node\n",
    "To connect the planner to our graph, we will create a get_plan node that accepts the ReWOO state and returns with a state update for the steps and plan_string fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Regex to match expressions of the form E#... = ...[...]\n",
    "regex_pattern = r\"Plan:\\s*(.+)\\s*(#E\\d+)\\s*=\\s*(\\w+)\\s*\\[([^\\]]+)\\]\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"user\", prompt)])\n",
    "planner = prompt_template | model\n",
    "\n",
    "\n",
    "def get_plan(state: ReWOO):\n",
    "    task = state[\"task\"]\n",
    "    result = planner.invoke({\"task\": task})\n",
    "    # Find all matches in the sample text\n",
    "    matches = re.findall(regex_pattern, result.content)\n",
    "    return {\"steps\": matches, \"plan_string\": result.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2a357",
   "metadata": {},
   "source": [
    "### Executor\n",
    "The executor receives the plan and executes the tools in sequence.\n",
    "\n",
    "Below, instantiate the search engine and define the tool execution node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac563fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f7f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_current_task(state: ReWOO):\n",
    "    if \"results\" not in state or state[\"results\"] is None:\n",
    "        return 1\n",
    "    if len(state[\"results\"]) == len(state[\"steps\"]):\n",
    "        return None\n",
    "    else:\n",
    "        return len(state[\"results\"]) + 1\n",
    "\n",
    "\n",
    "def tool_execution(state: ReWOO):\n",
    "    \"\"\"Worker node that executes the tools of a given plan.\"\"\"\n",
    "    _step = _get_current_task(state)\n",
    "    _, step_name, tool, tool_input = state[\"steps\"][_step - 1]\n",
    "    _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "    for k, v in _results.items():\n",
    "        tool_input = tool_input.replace(k, v)\n",
    "    if tool == \"Google\":\n",
    "        result = search.invoke(tool_input)\n",
    "    elif tool == \"LLM\":\n",
    "        result = model.invoke(tool_input)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    _results[step_name] = str(result)\n",
    "    return {\"results\": _results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26cb653",
   "metadata": {},
   "source": [
    "### Solver\n",
    "The solver receives the full plan and generates the final response based on the responses of the tool calls from the worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_prompt = \"\"\"\n",
    "Solve the following task or problem. To solve the problem, we have made step-by-step Plan and \\\n",
    "retrieved corresponding Evidence to each Plan. Use them with caution since long evidence might \\\n",
    "contain irrelevant information.\n",
    "\n",
    "{plan}\n",
    "\n",
    "Now solve the question or task according to provided Evidence above. Respond with the answer\n",
    "directly with no extra words.\n",
    "\n",
    "Task: {task}\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def solve(state: ReWOO):\n",
    "    plan = \"\"\n",
    "    for _plan, step_name, tool, tool_input in state[\"steps\"]:\n",
    "        _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "        for k, v in _results.items():\n",
    "            tool_input = tool_input.replace(k, v)\n",
    "            step_name = step_name.replace(k, v)\n",
    "        plan += f\"Plan: {_plan}\\n{step_name} = {tool}[{tool_input}]\"\n",
    "    prompt = solve_prompt.format(plan=plan, task=state[\"task\"])\n",
    "    result = model.invoke(prompt)\n",
    "    return {\"result\": result.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c6965",
   "metadata": {},
   "source": [
    "### Define Graph\n",
    "Our graph defines the workflow. Each of the planner, tool executor, and solver modules are added as nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79476610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _route(state):\n",
    "    _step = _get_current_task(state)\n",
    "    if _step is None:\n",
    "        # We have executed all tasks\n",
    "        return \"solve\"\n",
    "    else:\n",
    "        # We are still executing tasks, loop back to the \"tool\" node\n",
    "        return \"tool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6419e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "graph = StateGraph(ReWOO)\n",
    "graph.add_node(\"plan\", get_plan)\n",
    "graph.add_node(\"tool\", tool_execution)\n",
    "graph.add_node(\"solve\", solve)\n",
    "graph.add_edge(\"plan\", \"tool\")\n",
    "graph.add_edge(\"solve\", END)\n",
    "graph.add_conditional_edges(\"tool\", _route)\n",
    "graph.add_edge(START, \"plan\")\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427caa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in app.stream({\"task\": task}):\n",
    "    print(s)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01cbe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s[\"solve\"][\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
